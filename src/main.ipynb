{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra notes\n",
    "\n",
    "https://scikit-learn.org/stable/modules/neural_networks_supervised.html\n",
    "\n",
    "Hyper parameters for model:\n",
    "(hidden_layer_sizes: Any = (100, ), activation: str = \"relu\", *, solver: str = \"adam\", alpha: float = 0.0001, batch_size: str = \"auto\", learning_rate: str = \"constant\", learning_rate_init: float = 0.001, power_t: float = 0.5, max_iter: int = 200, shuffle: bool = True, random_state: Any | None = None, tol: float = 0.0001, verbose: bool = False, warm_start: bool = False, momentum: float = 0.9, nesterovs_momentum: bool = True, early_stopping: bool = False, validation_fraction: float = 0.1, beta_1: float = 0.9, beta_2: float = 0.999, epsilon: float = 1e-8, n_iter_no_change: int = 10, max_fun: int = 15000) -> None\n",
    "\n",
    "Relevant for project are: alpha, batch_size, learning_rate_init, momentum\n",
    "max_iter for epoch plot (if using adam solver)\n",
    "Alpha is for L2 regularization\n",
    "Might want to change learning_rate from constant\n",
    "\n",
    "activation functions we use: relu, tanh, logistic (sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from load_covid_data import load_covid_data\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from accuracy_score import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import warnings\n",
    "from GridSearch import *\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ignoring convergence warning for not bloating the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading covid dataset and splitting in train / test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Loading covid dataset\n",
    "headers, X, Y = load_covid_data()\n",
    "\n",
    "# Total amount of datapoints is 1 048 576\n",
    "n_datapoints = 100000\n",
    "\n",
    "X = X[: n_datapoints, :] \n",
    "Y = Y[: n_datapoints] \n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=[12, 10, 8], random_state=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(hidden_layer_sizes=[12, 10, 8], random_state=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=[12, 10, 8], random_state=1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "layers = [12,10,8]\n",
    "classifier = MLPClassifier(solver=\"adam\", hidden_layer_sizes=layers, random_state=1, activation=\"relu\")\n",
    "classifier.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing model with a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10269   815]\n",
      " [ 1392  7524]]\n",
      "0.88965\n"
     ]
    }
   ],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "print(accuracy_score(y_pred, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid searching on hyperparameters for the MLP on different activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:709: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    }
   ],
   "source": [
    "grid_search_hyperparameters_MLP(X_train, X_test, Y_train, Y_test, \"Accuracy score MLP (relu)\", func=\"relu\", verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'grid_search_hyperparameters_MLP' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m grid_search_hyperparameters_MLP(X_train, X_test, Y_train, Y_test, \u001b[39m\"\u001b[39m\u001b[39mAccuracy score MLP (logistic)\u001b[39m\u001b[39m\"\u001b[39m, func\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlogistic\u001b[39m\u001b[39m\"\u001b[39m, verbose \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'grid_search_hyperparameters_MLP' is not defined"
     ]
    }
   ],
   "source": [
    "grid_search_hyperparameters_MLP(X_train, X_test, Y_train, Y_test, \"Accuracy score MLP (logistic)\", func=\"logistic\", verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_hyperparameters_MLP(X_train, X_test, Y_train, Y_test, \"Accuracy score MLP (tanh)\", func=\"tanh\", verbose = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the MLP classifiers with the best alpha and momentum for each activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPClassifier(activation = \"relu\", max_iter = 10000, alpha = 0.0001, momentum = 1e-05)\n",
    "model.fit(X_train, Y_train)\n",
    "Y_pred = model.predict(X_test)\n",
    "print(accuracy_score(Y_pred, Y_test))\n",
    "\n",
    "model = MLPClassifier(activation = \"logistic\", max_iter = 10000, alpha = 0.0001, momentum = 1e-05)\n",
    "model.fit(X_train, Y_train)\n",
    "Y_pred = model.predict(X_test)\n",
    "print(accuracy_score(Y_pred, Y_test))\n",
    "\n",
    "model = MLPClassifier(activation = \"tanh\", max_iter = 10000, alpha = 0.0001, momentum = 1e-05)\n",
    "model.fit(X_train, Y_train)\n",
    "Y_pred = model.predict(X_test)\n",
    "print(accuracy_score(Y_pred, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that tanh activation function is performing marginly better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting accuracy score over epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 50\n",
    "\n",
    "predictions = np.zeros((max_epochs, 3))\n",
    "\n",
    "for i in range(1, max_epochs):\n",
    "    classifier_relu = MLPClassifier(solver=\"adam\", hidden_layer_sizes=layers, random_state=1, activation=\"relu\", max_iter = i, alpha = 0.0001, momentum = 1e-05)\n",
    "    classifier_relu.fit(X_train, Y_train)\n",
    "    predictions[i, 0] = accuracy_score(classifier.predict(X_test), Y_test, conf = False)\n",
    "\n",
    "    classifier_sigmoid = MLPClassifier(solver=\"adam\", hidden_layer_sizes=layers, random_state=1, activation=\"logistic\", max_iter = i, alpha = 0.0001, momentum = 1e-05)\n",
    "    classifier_sigmoid.fit(X_train, Y_train)\n",
    "    predictions[i, 1] = accuracy_score(classifier.predict(X_test), Y_test, conf = False)\n",
    "\n",
    "    classifier_tanh = MLPClassifier(solver=\"adam\", hidden_layer_sizes=layers, random_state=1, activation=\"tanh\", max_iter = i, alpha = 0.0001, momentum = 1e-05)\n",
    "    classifier_tanh.fit(X_train, Y_train)\n",
    "    predictions[i, 2] = accuracy_score(classifier.predict(X_test), Y_test, conf = False)\n",
    "\n",
    "# Plotting for relu\n",
    "plt.figure()\n",
    "plt.plot(predictions[:, 0] * 100)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy score\")\n",
    "plt.title(f\"Accuracy score over epochs with RELU activation function ({max_epochs} epochs) \")\n",
    "plt.savefig(\"../figures/accuracy_over_epochs_mlp_relu\")\n",
    "plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter())\n",
    "\n",
    "# Plotting for sigmoid\n",
    "plt.figure()\n",
    "plt.plot(predictions[:, 1] * 100)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy score\")\n",
    "plt.title(f\"Accuracy score over epochs with sigmoid activation function ({max_epochs} epochs)\")\n",
    "plt.savefig(\"../figures/accuracy_over_epochs_mlp_relu\")\n",
    "plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter())\n",
    "\n",
    "# Plotting for tanh\n",
    "plt.figure()\n",
    "plt.plot(predictions[:, 2] * 100)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy score\")\n",
    "plt.title(f\"Accuracy score over epochs with tanh activation function ({max_epochs} epochs)\")\n",
    "plt.savefig(\"../figures/accuracy_over_epochs_mlp_relu\")\n",
    "plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forests method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(random_state=1)\n",
    "forest.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a prediction with the random forest method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = forest.predict(X_test)\n",
    "print(accuracy_score(Y_test, Y_pred))\n",
    "print(\"predicted chance of dying:\",np.sum(Y_pred)/len(Y_pred))\n",
    "print(\"Actual chance of dying (test set):\",np.sum(Y_test)/len(Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "importances = forest.feature_importances_\n",
    "std = np.std(importances)\n",
    "forest_importances = pd.Series(importances, index=headers)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "forest_importances.plot.bar(yerr=std, ax=ax)\n",
    "ax.set_title(\"Feature importances using Forest of Trees\")\n",
    "ax.set_ylabel(\"Importance in %\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Making histogram of amount of deaths given a certain age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age = X[:,6]\n",
    "deaths = {i:0 for i in np.unique(age)}\n",
    "for i in range(len(Y)):\n",
    "    deaths[age[i]] += Y[i]\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(range(len(deaths)), list(deaths.values()))\n",
    "plt.ylabel(\"Number of deaths\")\n",
    "plt.xlabel(\"Age\")\n",
    "\n",
    "#Making histogram of chance of dying given a certain age\n",
    "\n",
    "chance_dying = deaths.copy()\n",
    "amount = {i:0 for i in np.unique(age)}\n",
    "\n",
    "for i in range(len(Y)):\n",
    "    amount[age[i]] += 1\n",
    "\n",
    "for key in chance_dying:\n",
    "    chance_dying[key] /= amount[key]\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(range(len(chance_dying)), list(chance_dying.values()))\n",
    "plt.ylabel(\"Chance of dying\")\n",
    "plt.xlabel(\"Age\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking into chance of dyaing based on age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note, not finished, need to figure out way to fix imbalanced data\n",
    "\n",
    "Age_train = X_train[:,6].reshape(-1,1)\n",
    "Age_test = X_test[:,6].reshape(-1,1)\n",
    "\n",
    "model = MLPClassifier(activation=\"logistic\", max_iter=10000)\n",
    "model.fit(Age_train, Y_train)\n",
    "YA_pred = model.predict(Age_test)\n",
    "print(accuracy_score(YA_pred, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forestAge = RandomForestClassifier(random_state=1)\n",
    "forestAge.fit(Age_train, Y_train)\n",
    "pred = forestAge.predict(Age_test)\n",
    "accuracy_score(pred, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems we arent able to classify based on just age. This makes sence since every percentage based on age is lower than 50, so\n",
    "our classifier would not benefit from setting any given age as death.\n",
    "We therefore try with polynomial fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ChanceDying(X,Y):\n",
    "    ages = np.unique(X)\n",
    "    deaths = {i:0 for i in ages}\n",
    "    numb = deaths.copy()\n",
    "    for i in range(len(Y)):\n",
    "        deaths[X[i]] += Y[i]\n",
    "        numb[X[i]] += 1\n",
    "\n",
    "\n",
    "    for key in deaths:\n",
    "        if numb[key] > 0:\n",
    "            deaths[key] /= numb[key] \n",
    "    \n",
    "    return deaths \n",
    "\n",
    "def MakeArray(dict):\n",
    "    length = len(dict)\n",
    "    x_arr = np.zeros(length)\n",
    "    y_arr = np.zeros(length)\n",
    "\n",
    "    for i,key in enumerate(dict):\n",
    "        x_arr[i] = key \n",
    "        y_arr[i] = dict[key]\n",
    "\n",
    "    return x_arr, y_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Age = X[:,6]\n",
    "M = len(Age) #chosen number of data points\n",
    "n = len(Age)\n",
    "m = int(n/M)\n",
    "random_index = np.random.randint(m)*M   \n",
    "new_Age = Age[random_index:random_index+M]\n",
    "new_Y = Y[random_index:random_index+M]\n",
    "\n",
    "Age_train, Age_test, YA_train, YA_test = train_test_split(new_Age, new_Y, test_size=0.2)\n",
    "\n",
    "deaths_train = ChanceDying(Age_train, YA_train)\n",
    "probx_train, proby_train = MakeArray(deaths_train)\n",
    "\n",
    "deaths_test = ChanceDying(Age_test, YA_test)\n",
    "probx_test, proby_test = MakeArray(deaths_test)\n",
    "\n",
    "#Testing to find optimal polynomial degree\n",
    "for i in range(20):\n",
    "    lin = LinearRegression()\n",
    "    poly = PolynomialFeatures(degree=i)\n",
    "    X_poly = poly.fit_transform(probx_train.reshape(-1,1))\n",
    "    lin.fit(X_poly, proby_train)\n",
    "\n",
    "    X_poly_test = poly.fit_transform(probx_test.reshape(-1,1))\n",
    "    print(i, MSE(proby_test, lin.predict(X_poly_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing multiple runs it seems around 10 is the most stable amount of polynomials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin = LinearRegression()\n",
    "poly = PolynomialFeatures(degree=10)\n",
    "X_poly = poly.fit_transform(probx_train.reshape(-1,1))\n",
    "lin.fit(X_poly, proby_train)\n",
    "\n",
    "X_poly_test = poly.fit_transform(probx_test.reshape(-1,1))\n",
    "\n",
    "plt.plot(probx_test, lin.predict(X_poly_test))\n",
    "plt.bar(probx_test, proby_test)\n",
    "plt.show()\n",
    "\n",
    "plt.bar(probx_train, proby_train)\n",
    "plt.plot(probx_train, lin.predict(X_poly))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = lin.predict(X_poly_test)\n",
    "length = len(pred)\n",
    "X_true, ans = MakeArray(chance_dying)\n",
    "print(MSE(ans[:length], pred))\n",
    "\n",
    "\n",
    "length = 100\n",
    "X_poly_true = poly.fit_transform(X_true.reshape(-1,1))\n",
    "plt.plot(X_true[:length], lin.predict(X_poly_true[:length]))\n",
    "plt.bar(X_true, ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now testing for different sample sizes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for M in [100,1000,10000, 50000, 100000, 200000, 500000]:\n",
    "    Age = X[:,6]\n",
    "    \n",
    "    #Shuffling data because it seems not random enough for good resampling\n",
    "    Age, new_Y = shuffle(Age,Y)\n",
    "\n",
    "    #M = len(Age) #chosen number of data points\n",
    "    n = len(Age)\n",
    "    m = int(n/M)\n",
    "    random_index = np.random.randint(m)*M   \n",
    "    new_Age = Age[random_index:random_index+M]\n",
    "    new_Y = new_Y[random_index:random_index+M]\n",
    "\n",
    "    Age_train, Age_test, YA_train, YA_test = train_test_split(new_Age, new_Y, test_size=0.2)\n",
    "\n",
    "    deaths_train = ChanceDying(Age_train, YA_train)\n",
    "    probx_train, proby_train = MakeArray(deaths_train)\n",
    "\n",
    "    deaths_test = ChanceDying(Age_test, YA_test)\n",
    "    probx_test, proby_test = MakeArray(deaths_test)\n",
    "\n",
    "    lin = LinearRegression()\n",
    "    poly = PolynomialFeatures(degree=10)\n",
    "    X_poly = poly.fit_transform(probx_train.reshape(-1,1))\n",
    "    lin.fit(X_poly, proby_train)\n",
    "\n",
    "    X_poly_test = poly.fit_transform(probx_test.reshape(-1,1))\n",
    "    pred = lin.predict(X_poly_test)\n",
    "    plt.plot(probx_test, pred, label=f\"Sample size = {M}\")\n",
    "\n",
    "    print(f\"MSE for sample size {M} is:\",MSE(ans[:len(pred)], pred))\n",
    "\n",
    "    #length = 100\n",
    "    #X_poly_true = poly.fit_transform(X_true.reshape(-1,1))\n",
    "    #plt.plot(X_true[:length], lin.predict(X_poly_true[:length]))\n",
    "    \n",
    "\n",
    "plt.legend()\n",
    "plt.ylim(0,0.5)\n",
    "plt.bar(X_true, ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test code, use later for removing features\n",
    "\n",
    "A = [1,2,2,2,3,3,5,5]\n",
    "A = np.array(A)\n",
    "remove = np.where(A > 2)\n",
    "A = np.delete(A, remove)\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = np.corrcoef(X)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(correlation_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "82a8583af3f5fca3e5e666e9f759009d90258d78e095848e209b6f7ca5a474b0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
